import pathlib

from fastapi import FastAPI
from fastapi.exceptions import HTTPException

from inference_server.config import settings
from inference_server.models import Model
from inference_server.schemas import InferencePayload, InferenceResult

app = FastAPI()
models: dict[str, Model] = dict()


@app.get("/api/v1/list")
def list_models():
    return list(map(lambda x: x.stem, settings.models_dir.glob("*.onnx")))


@app.post("/api/v1/load")
def load_model(name: str):
    """Loads model with name `name.onnx` in `models_dir`.

    Args:
        name (str): Name of the model to load, without the .onnx suffix.

    """
    model_path = settings.models_dir / f"{name}.onnx"
    if not model_path.exists():
        raise HTTPException(
            status_code=404, detail=f"Model with name {name} not found."
        )

    if name not in models:
        models[name] = Model(model_path, autoload=True)

    return 200


@app.post("/api/v1/run/{name}")
def run_model(name: str, payload: InferencePayload):
    """ """
    if name not in models:
        raise HTTPException(
            status_code=409, detail=f"No model with name {name} has been loaded."
        )

    result = models[name].from_bytes(payload.image)
    return InferenceResult(category=result)


@app.post("/api/v1/unload")
def unload_model(name: str):
    """ """
    if name not in models:
        raise HTTPException(
            status_code=409, detail=f"No model with name {name} has been loaded."
        )

    del models[name]

    return 200


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=3000)
