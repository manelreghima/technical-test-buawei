import os
from typing import List
import numpy as np
import onnxruntime as ort
import onnx
from fastapi import FastAPI, File, UploadFile, HTTPException,Query,Depends, Request,APIRouter

from inference_server.config import settings
from inference_server.models import Model
from inference_server.schemas import InferencePayload, InferenceResult

from pydantic import BaseModel

app = FastAPI()
models: dict[str, Model] = dict()

@app.get("/api/v1/list")
def list_models():
    return list(map(lambda x: x.stem, settings.models_dir.glob("*.onnx")))

@app.post("/api/v1/load")
def load_model(name: str):
    model_path = settings.models_dir / f"{name}.onnx"
    if not model_path.exists():
        raise HTTPException(
            status_code=404, detail=f"Model with name {name} not found."
        )
    if name not in models:
        models[name] = Model(model_path, autoload=True)
    return 200

@app.post("/api/v1/run/{name}")
def run_model(name: str, payload: InferencePayload):
    if name not in models:
        raise HTTPException(
            status_code=409, detail=f"No model with name {name} has been loaded."
        )
    result = models[name].from_bytes(payload.image)
    return InferenceResult(category=result)

@app.post("/api/v1/unload")
def unload_model(name: str):
    if name not in models:
        raise HTTPException(
            status_code=409, detail=f"No model with name {name} has been loaded."
        )
    del models[name]
    return 200



class RegisterModel(BaseModel):
    model_name: str

@app.post("/api/v1/register")
async def register_model(model_name: str, file: UploadFile = File(...)):
    if file.filename.endswith('.onnx'):
        file_location = f"./models/{model_name}.onnx"
        
        if os.path.exists(file_location):
            raise HTTPException(status_code=400, detail="Model already exists.")
        
        with open(file_location, "wb+") as file_object:
            file_object.write(await file.read())  # added await for asynchronous file read
        
        # Validate ONNX model
        try:
            onnx_model = onnx.load(file_location)
            onnx.checker.check_model(onnx_model)
        except Exception as e:
            os.remove(file_location)  # Cleanup if validation fails
            raise HTTPException(status_code=422, detail=str(e))
        
        # Here you could update the model registry/database with new model's metadata
        
        return {"message": "Model uploaded and validated successfully"}
    else:
        raise HTTPException(status_code=400, detail="Invalid file type.")
    
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=3000)
